bundle:
  name: oos-workflow

# Define wheel artifact (use pre-built wheel, don't build during deploy)
artifacts:
  default:
    type: whl
    path: .
    build: uv build

# Define Databricks resources
resources:
  jobs:
    oos_workflow_job:
      name: "OOS Workflow Job"
      
      # Use existing compute cluster
      # Alternatively, uncomment job_clusters below to create ephemeral cluster
      # job_clusters:
      #   - job_cluster_key: "oos_cluster"
      #     new_cluster:
      #       spark_version: "13.3.x-scala2.12"
      #       node_type_id: "Standard_DS3_v2"
      #       num_workers: 1
      
      # Define workflow tasks
      tasks:
        - task_key: "run_workflow"
          existing_cluster_id: "<Cluster-ID>"  # Test ADB Container Application Cluster
          
          # Python wheel task configuration
          python_wheel_task:
            package_name: "oos_workflow"
            entry_point: "main"
            parameters: ["--env", "databricks"]
          
          # Install wheel package as library
          libraries:
            - whl: ./dist/*.whl
      
      # Job settings
      max_concurrent_runs: 1
      timeout_seconds: 3600
      email_notifications:
        on_failure: []
        on_success: []

# Define deployment targets
targets:
  dev:
    mode: development
    workspace:
      host: https://adb-<Workspace-ID>.azuredatabricks.net
      # Note: Authentication should be configured via Databricks CLI
      # Run: databricks configure --token
    
    resources:
      jobs:
        oos_workflow_job:
          # Override settings for dev environment
          name: "OOS Workflow Job (Dev)"
  
  prod:
    mode: production
    workspace:
      host: https://adb-<Workspace-ID>.azuredatabricks.net
    
    resources:
      jobs:
        oos_workflow_job:
          name: "OOS Workflow Job (Prod)"
          # Production-specific settings
          max_concurrent_runs: 3
          email_notifications:
            on_failure: ["<Email-Address>"]
